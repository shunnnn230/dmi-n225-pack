name: build-and-publish-n225-pack
on:
  schedule:
    - cron: "10 7 * * *"   # 07:10 UTC = 16:10 JST
  workflow_dispatch:
permissions:
  contents: read
  pages: write
  id-token: write
concurrency:
  group: "pages"
  cancel-in-progress: false
jobs:
  build:
    runs-on: ubuntu-latest
    env: { TZ: "Asia/Tokyo", CI: "1" }
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with: { python-version: "3.11" }
      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt || true
          # フォールバック用に最低限は必ず入れる
          pip install yfinance ta pandas numpy
      - name: Run batch
        run: python n225_batch.py || true

      # ★ここが保険：出力が空なら代表銘柄でミニパックを生成
      - name: Sanity check & fallback
        run: |
          python - <<'PY'
          import os, sys, json, time, csv
          import pandas as pd, numpy as np
          import yfinance as yf
          try:
              from ta.trend import ADXIndicator
          except Exception:
              ADXIndicator = None

          base='out/n225'
          os.makedirs(base, exist_ok=True)
          met=os.path.join(base,'latest_metrics.csv')
          need_fallback=True
          if os.path.exists(met):
              try:
                  df=pd.read_csv(met)
                  if len(df)>0 and (df['trades'].fillna(0).astype(int).sum()>0):
                      need_fallback=False
                      print('[OK] metrics 既に有効。フォールバック不要')
              except Exception as e:
                  print('[WARN] metrics 読み込み失敗:', e)

          if not need_fallback:
              sys.exit(0)

          print('[INFO] metrics が空 → フォールバック生成')
          TICKERS=['8035.T','7203.T','6758.T','9984.T','9983.T','6954.T','4502.T','4063.T','6367.T','7267.T','9432.T','9433.T','9434.T','7751.T']
          rows=[]; signals={}
          for t in TICKERS:
              try:
                  data=yf.download(t,period='5y',interval='1d',auto_adjust=True,progress=False,threads=False)
                  if len(data)<30: 
                      print('[SKIP] 短すぎ:',t); continue
                  ret=(data['Close'].iloc[-1]/data['Close'].iloc[0]-1)*100.0
                  years=max(1.0,(data.index[-1]-data.index[0]).days/365.25)
                  cagr=((data['Close'].iloc[-1]/data['Close'].iloc[0])**(1/years)-1)*100.0
                  sig,reason='HOLD','NONE'
                  if ADXIndicator is not None:
                      try:
                          adx=ADXIndicator(high=data['High'],low=data['Low'],close=data['Close'],window=14,fillna=False)
                          S=(adx.adx_pos()-adx.adx_neg())
                          is_gc=(S.iloc[-2]<=0 and S.iloc[-1]>0)
                          is_dc=(S.iloc[-2]>=0 and S.iloc[-1]<0)
                          sig='BUY' if is_gc else ('SELL' if is_dc else 'HOLD'); reason='GC' if is_gc else ('DC' if is_dc else 'NONE')
                      except Exception: pass
                  rows.append(dict(ticker=t,name_jp='',mgc='',mdc='',trades=1,win_rate=np.nan,total_return=ret,cagr=cagr,max_dd=np.nan,
                                   start=str(data.index[0].date()),end=str(data.index[-1].date()),
                                   updated_at=time.strftime('%Y-%m-%dT%H:%M:%S+09:00',time.localtime())))
                  signals[t]=dict(signal=sig,reason=reason)
              except Exception as e:
                  print('[ERR]',t,e)

          if not rows:
              print('[FATAL] フォールバックも0件'); sys.exit(1)

          # CSV
          with open(met,'w',newline='',encoding='utf-8') as f:
              w=csv.DictWriter(f,fieldnames=['ticker','name_jp','mgc','mdc','trades','win_rate','total_return','cagr','max_dd','start','end','updated_at'])
              w.writeheader()
              for r in rows: w.writerow(r)

          # パック（Top10等）
          rows_sorted=sorted(rows,key=lambda r: (r.get('total_return') or -1e9), reverse=True)
          top=[{'ticker':r['ticker'],'name_jp':r['name_jp'] or r['ticker'],'total_pct':r['total_return'],'cagr_pct':r['cagr']} for r in rows_sorted]
          pack=dict(updated_at=time.strftime('%Y-%m-%dT%H:%M:%S+09:00',time.localtime()), top=top, stars={}, signals=signals, metrics={})
          with open(os.path.join(base,'latest_pack.json'),'w',encoding='utf-8') as f: json.dump(pack,f,ensure_ascii=False)
          pd.DataFrame([dict(ticker=k, **v) for k,v in signals.items()]).to_csv(os.path.join(base,'latest_signals.csv'), index=False)
          print('[DONE] フォールバック生成:',len(rows),'tickers')
          PY

      - name: Make public artifacts
        run: |
          mkdir -p public
          cp -v out/n225/latest_pack.json    public/
          cp -v out/n225/version.json        public/  || true
          cp -v out/n225/latest_metrics.csv  public/
          cp -v out/n225/latest_signals.csv  public/

      - name: Write checksummed version.json
        run: |
          python - <<'PY'
          import hashlib, json, os, time
          base='public'
          def sha(p):
              h=hashlib.sha256()
              with open(p,'rb') as f:
                  for b in iter(lambda: f.read(262144), b''): h.update(b)
              return h.hexdigest(), os.path.getsize(p)
          files = ['latest_pack.json','latest_metrics.csv','latest_signals.csv']
          out = {}
          for f in files:
              he, sz = sha(os.path.join(base,f))
              key = f.replace('latest_','').replace('.','_')
              out[key+'_sha256'] = he
              out[key+'_size']   = sz
          out['updated_at'] = time.strftime('%Y-%m-%dT%H:%M:%S+09:00', time.localtime())
          with open(os.path.join(base,'version.json'),'w',encoding='utf-8') as g:
              json.dump(out, g, ensure_ascii=False)
          PY

      - name: Upload artifact (public/*)
        uses: actions/upload-pages-artifact@v3
        with: { path: public }

  deploy:
    needs: build
    runs-on: ubuntu-latest
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    steps:
      - id: deployment
        uses: actions/deploy-pages@v4
